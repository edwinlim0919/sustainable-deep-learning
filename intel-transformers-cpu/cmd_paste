ray start --head

serve build int4_llama2_model:llama2_endpoint -o ../configs/ray_llama2.yaml
serve run ../configs/ray_llama2.yaml
python3 int4_llama2_client.py

ray stop




# Flow from https://docs.ray.io/en/latest/serve/production-guide/index.html
serve build int4_llama2_model:llama2_endpoint -o serve_config.yaml
ray start --head
serve deploy serve_config.yaml
python3 int4_llama2_client.py




# Single-node cluster
serve build int4_llama2_model:llama2_endpoint -o serve_config.yaml
ray start --head --port=6379
serve deploy serve_config.yaml
python3 int4_llama2_client.py


# Ray is hot garbage for CPU inference
python3 int4_llama2_local.py --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json --output-file-path c6420_1_1000_10_05_3_5.txt --head-node-ip http://130.127.133.221:8000/ --num-requests-sample 1000 --requests-per-rate 10 --start-rate 0.5 --end-rate 3 --increase-rate 5

python3 int4_llama2_local.py --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json --output-file-path c6420_1_1000_20_05_10_2.txt --num-requests-sample 1000 --requests-per-rate 20 --start-rate 0.5 --end-rate 10 --increase-rate 2

python3 int4_llama2_local.py --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json --output-file-path c6420_1_1000_25_05_10_2.txt --num-requests-sample 1000 --requests-per-rate 25 --start-rate 0.5 --end-rate 10 --increase-rate 2

python3 int4_llama2_local.py --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json --output-file-path c6420_1_1000_1_05_10_2.txt --num-requests-sample 1000 --requests-per-rate 1 --start-rate 0.5 --end-rate 10 --increase-rate 2

ps aux | grep int4_llama2_local.py | grep -v grep | awk '{print $2}' | xargs kill
